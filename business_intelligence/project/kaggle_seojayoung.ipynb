{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kaggle (Two Sigma Connect: Rental Listing Inquiries) 대회 참가 후기\n",
    "## 성능 및 등수 - Private : 0.53109 (620/2488) / Public - 0.53136 (637/2488)\n",
    "<img src=\"https://github.com/jayoungseo/quiz-image/blob/master/private.PNG?raw=true\"></img>\n",
    "<img src=\"https://github.com/jayoungseo/quiz-image/blob/master/public.PNG?raw=true\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모듈과 데이터 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "\n",
    "train_df = pd.read_json(\"./data/train.json\").sort_values(by=\"listing_id\")\n",
    "test_df = pd.read_json(\"./data/test.json\").sort_values(by=\"listing_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 feature 생성 1\n",
    "- logprice : price에 log를 취해준것\n",
    "- price_t : price를 bedrooms의 개수로 나눠준 것\n",
    "- room_sum : bedrooms의 개수와 bathrooms 개수를 더해준 것\n",
    "- price_per_room : price를 bedrooms의 개수와 bathrooms 개수를 더해준 수로 나눠준 것\n",
    "- num_photos : 방이 가지고 있는 사진의 개수\n",
    "- num_features : 방 마다 가진 특징의 개수\n",
    "- num_description_words : 방을 설명한 단어의 개수\n",
    "- created : 데이터 생성 시간\n",
    "- created_year, created_month, created_day, created_hour : 데이터 생성 시간을 년,월,일,시간으로 나눈 것\n",
    "- pos : ...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df[\"bathrooms\"].loc[19671] = 1.5\n",
    "test_df[\"bathrooms\"].loc[22977] = 2.0\n",
    "test_df[\"bathrooms\"].loc[63719] = 2.0\n",
    "train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "\n",
    "train_df[\"logprice\"] = np.log(train_df[\"price\"])\n",
    "test_df[\"logprice\"] = np.log(test_df[\"price\"])\n",
    "\n",
    "train_df[\"price_t\"] =train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "test_df[\"price_t\"] = test_df[\"price\"]/test_df[\"bedrooms\"] \n",
    "\n",
    "train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "test_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n",
    "\n",
    "train_df['price_per_room'] = train_df['price']/train_df['room_sum']\n",
    "test_df['price_per_room'] = test_df['price']/test_df['room_sum']\n",
    "\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "train_df[\"created_dayofyear\"] = train_df[\"created\"].dt.dayofyear\n",
    "test_df[\"created_dayofyear\"] = test_df[\"created\"].dt.dayofyear\n",
    "\n",
    "train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "features_to_use=[\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\"price_t\",\"price_per_room\", \"logprice\", \"density\"\n",
    "                 ,\"num_photos\", \"num_features\", \"num_description_words\",\"listing_id\", \"created_year\", \"created_month\"\n",
    "                 , \"created_day\", \"created_hour\", \"created_dayofyear\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 feature 생성 2\n",
    "- 위도(latitude)와 경도(longitude)를 활용한 새로운 feature 추가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cart2rho(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    return rho\n",
    "\n",
    "\n",
    "def cart2phi(x, y):\n",
    "    phi = np.arctan2(y, x)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def rotation_x(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return x*math.cos(alpha) + y*math.sin(alpha)\n",
    "\n",
    "\n",
    "def rotation_y(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return y*math.cos(alpha) - x*math.sin(alpha)\n",
    "\n",
    "\n",
    "def add_rotation(degrees, df):\n",
    "    namex = \"rot\" + str(degrees) + \"_X\"\n",
    "    namey = \"rot\" + str(degrees) + \"_Y\"\n",
    "\n",
    "    df['num_' + namex] = df.apply(lambda row: rotation_x(row, math.pi/(180/degrees)), axis=1)\n",
    "    df['num_' + namey] = df.apply(lambda row: rotation_y(row, math.pi/(180/degrees)), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def operate_on_coordinates(tr_df, te_df):\n",
    "    for df in [tr_df, te_df]:\n",
    "        #polar coordinates system\n",
    "        df[\"num_rho\"] = df.apply(lambda x: cart2rho(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        df[\"num_phi\"] = df.apply(lambda x: cart2phi(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        #rotations\n",
    "        for angle in [15,30,45,60]:\n",
    "            df = add_rotation(angle, df)\n",
    "\n",
    "    return tr_df, te_df\n",
    "\n",
    "train_df, test_df = operate_on_coordinates(train_df, test_df)\n",
    "\n",
    "features_to_use.append(\"num_rho\")\n",
    "features_to_use.append(\"num_phi\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 feature 생성 3\n",
    "- description에 포함된 요소에 따라 점수를 주는 feature 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seojayoung/miniconda3/envs/ml_python/lib/python3.5/site-packages/pandas/core/indexing.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cap_share(x):\n",
    "    return sum(1 for c in x if c.isupper())/float(len(x)+1)\n",
    "\n",
    "for df in [train_df, test_df]:\n",
    "    # do you think that users might feel annoyed BY A DESCRIPTION THAT IS SHOUTING AT THEM?\n",
    "    df['num_cap_share'] = df['description'].apply(cap_share)\n",
    "    \n",
    "    # how long in lines the desc is?\n",
    "    df['num_nr_of_lines'] = df['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "   \n",
    "    # is the description redacted by the website?        \n",
    "    df['num_redacted'] = 0\n",
    "    df['num_redacted'].ix[df['description'].str.contains('website_redacted')] = 1\n",
    "\n",
    "    \n",
    "    # can we contact someone via e-mail to ask for the details?\n",
    "    df['num_email'] = 0\n",
    "    df['num_email'].ix[df['description'].str.contains('@')] = 1\n",
    "    \n",
    "    #and... can we call them?\n",
    "    \n",
    "    reg = re.compile(\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", re.S)\n",
    "    def try_and_find_nr(description):\n",
    "        if reg.match(description) is None:\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    df['num_phone_nr'] = df['description'].apply(try_and_find_nr)\n",
    "    \n",
    "features_to_use.append(\"num_phone_nr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 feature 생성 4\n",
    "- manage_id와 interest_level을 활용해 manager_level을 low, medium, high로 나눔 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    \n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    \n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "            \n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "            \n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])\n",
    "\n",
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB 모델 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=2000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.02\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=25)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB 모델을 이용하여 학습\n",
    "- 최종 loss 값 : 0.517707"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08485\ttest-mlogloss:1.08516\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[25]\ttrain-mlogloss:0.848793\ttest-mlogloss:0.856119\n",
      "[50]\ttrain-mlogloss:0.726\ttest-mlogloss:0.738717\n",
      "[75]\ttrain-mlogloss:0.655599\ttest-mlogloss:0.672894\n",
      "[100]\ttrain-mlogloss:0.612746\ttest-mlogloss:0.633855\n",
      "[125]\ttrain-mlogloss:0.584744\ttest-mlogloss:0.609401\n",
      "[150]\ttrain-mlogloss:0.564601\ttest-mlogloss:0.592548\n",
      "[175]\ttrain-mlogloss:0.549451\ttest-mlogloss:0.580479\n",
      "[200]\ttrain-mlogloss:0.537639\ttest-mlogloss:0.571818\n",
      "[225]\ttrain-mlogloss:0.527788\ttest-mlogloss:0.564889\n",
      "[250]\ttrain-mlogloss:0.519187\ttest-mlogloss:0.559364\n",
      "[275]\ttrain-mlogloss:0.511568\ttest-mlogloss:0.555023\n",
      "[300]\ttrain-mlogloss:0.504385\ttest-mlogloss:0.5512\n",
      "[325]\ttrain-mlogloss:0.498152\ttest-mlogloss:0.548209\n",
      "[350]\ttrain-mlogloss:0.492444\ttest-mlogloss:0.545448\n",
      "[375]\ttrain-mlogloss:0.486758\ttest-mlogloss:0.543011\n",
      "[400]\ttrain-mlogloss:0.481999\ttest-mlogloss:0.540986\n",
      "[425]\ttrain-mlogloss:0.477292\ttest-mlogloss:0.539209\n",
      "[450]\ttrain-mlogloss:0.472438\ttest-mlogloss:0.537346\n",
      "[475]\ttrain-mlogloss:0.467975\ttest-mlogloss:0.535835\n",
      "[500]\ttrain-mlogloss:0.463585\ttest-mlogloss:0.534491\n",
      "[525]\ttrain-mlogloss:0.459417\ttest-mlogloss:0.53327\n",
      "[550]\ttrain-mlogloss:0.455549\ttest-mlogloss:0.532062\n",
      "[575]\ttrain-mlogloss:0.451668\ttest-mlogloss:0.531029\n",
      "[600]\ttrain-mlogloss:0.447828\ttest-mlogloss:0.530056\n",
      "[625]\ttrain-mlogloss:0.444275\ttest-mlogloss:0.529231\n",
      "[650]\ttrain-mlogloss:0.440698\ttest-mlogloss:0.528415\n",
      "[675]\ttrain-mlogloss:0.437078\ttest-mlogloss:0.527757\n",
      "[700]\ttrain-mlogloss:0.43365\ttest-mlogloss:0.527082\n",
      "[725]\ttrain-mlogloss:0.430625\ttest-mlogloss:0.526489\n",
      "[750]\ttrain-mlogloss:0.427438\ttest-mlogloss:0.525985\n",
      "[775]\ttrain-mlogloss:0.424187\ttest-mlogloss:0.525431\n",
      "[800]\ttrain-mlogloss:0.42126\ttest-mlogloss:0.524951\n",
      "[825]\ttrain-mlogloss:0.417953\ttest-mlogloss:0.524343\n",
      "[850]\ttrain-mlogloss:0.415194\ttest-mlogloss:0.52385\n",
      "[875]\ttrain-mlogloss:0.412243\ttest-mlogloss:0.523363\n",
      "[900]\ttrain-mlogloss:0.409622\ttest-mlogloss:0.52301\n",
      "[925]\ttrain-mlogloss:0.406791\ttest-mlogloss:0.522684\n",
      "[950]\ttrain-mlogloss:0.403969\ttest-mlogloss:0.522238\n",
      "[975]\ttrain-mlogloss:0.401188\ttest-mlogloss:0.52195\n",
      "[1000]\ttrain-mlogloss:0.398288\ttest-mlogloss:0.521671\n",
      "[1025]\ttrain-mlogloss:0.395628\ttest-mlogloss:0.521281\n",
      "[1050]\ttrain-mlogloss:0.393002\ttest-mlogloss:0.520899\n",
      "[1075]\ttrain-mlogloss:0.390505\ttest-mlogloss:0.520593\n",
      "[1100]\ttrain-mlogloss:0.387793\ttest-mlogloss:0.520343\n",
      "[1125]\ttrain-mlogloss:0.385346\ttest-mlogloss:0.5201\n",
      "[1150]\ttrain-mlogloss:0.382895\ttest-mlogloss:0.519935\n",
      "[1175]\ttrain-mlogloss:0.380294\ttest-mlogloss:0.519728\n",
      "[1200]\ttrain-mlogloss:0.37781\ttest-mlogloss:0.519581\n",
      "[1225]\ttrain-mlogloss:0.375384\ttest-mlogloss:0.519394\n",
      "[1250]\ttrain-mlogloss:0.373014\ttest-mlogloss:0.519276\n",
      "[1275]\ttrain-mlogloss:0.37069\ttest-mlogloss:0.519073\n",
      "[1300]\ttrain-mlogloss:0.368569\ttest-mlogloss:0.518915\n",
      "[1325]\ttrain-mlogloss:0.366205\ttest-mlogloss:0.518722\n",
      "[1350]\ttrain-mlogloss:0.363747\ttest-mlogloss:0.518527\n",
      "[1375]\ttrain-mlogloss:0.361344\ttest-mlogloss:0.518383\n",
      "[1400]\ttrain-mlogloss:0.359119\ttest-mlogloss:0.518215\n",
      "[1425]\ttrain-mlogloss:0.356759\ttest-mlogloss:0.518174\n",
      "[1450]\ttrain-mlogloss:0.354459\ttest-mlogloss:0.518047\n",
      "[1475]\ttrain-mlogloss:0.352126\ttest-mlogloss:0.517945\n",
      "[1500]\ttrain-mlogloss:0.350019\ttest-mlogloss:0.517816\n",
      "[1525]\ttrain-mlogloss:0.347766\ttest-mlogloss:0.517705\n",
      "[1550]\ttrain-mlogloss:0.345802\ttest-mlogloss:0.517633\n",
      "[1575]\ttrain-mlogloss:0.343535\ttest-mlogloss:0.517655\n",
      "[1600]\ttrain-mlogloss:0.341569\ttest-mlogloss:0.517609\n",
      "[1625]\ttrain-mlogloss:0.33933\ttest-mlogloss:0.517712\n",
      "Stopping. Best iteration:\n",
      "[1581]\ttrain-mlogloss:0.343022\ttest-mlogloss:0.517569\n",
      "\n",
      "[0.51770798097998549]\n"
     ]
    }
   ],
   "source": [
    "cv_scores = []\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n",
    "        \n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        print(cv_scores)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision, recall, f1-score, support 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.37      0.61      0.46       458\n",
      "          1       0.44      0.55      0.49      1854\n",
      "          2       0.92      0.83      0.88      7559\n",
      "\n",
      "avg / total       0.81      0.77      0.78      9871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred_y = np.array([np.argmax(i) for i in preds])\n",
    "pred_y, y = pd.Series(pred_y), pd.Series(val_y)\n",
    "\n",
    "print(classification_report(pred_y, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습시킨 데이터 CSV 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=2000)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"sub_test2.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
