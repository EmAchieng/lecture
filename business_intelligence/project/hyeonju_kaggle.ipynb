{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle - Two Sigma Connect : Rental Listing Inguiries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeonju/anaconda3/envs/ml_python/lib/python3.4/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"../input/train.json\").sort_values('listing_id')\n",
    "test_df = pd.read_json(\"../input/test.json\").sort_values('listing_id')\n",
    "image_date = pd.read_csv(\"../input/listing_image_time.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def runRandomForest(x_data, y_data):\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2)\n",
    "\n",
    "    rfmodel = RandomForestClassifier(n_estimators=1000)\n",
    "    rfmodel.fit(x_train, y_train)\n",
    "\n",
    "    y_val_pred = rfmodel.predict_proba(x_val)\n",
    "    \n",
    "    print(log_loss(y_val, y_val_pred))\n",
    "    \n",
    "    return y_val, y_val_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=0, num_rounds=4000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.03\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    param['nthread'] = -1\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=25)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "\n",
    "def train_xgboost(train_X, train_y):\n",
    "    cv_scores = []\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "    for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_y, val_X, val_y)\n",
    "        cv_scores.append(log_loss(val_y, preds))\n",
    "        break\n",
    "        \n",
    "    return val_y, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def predict(train_X, train_y, test_X, filename, rounds=1300):\n",
    "\n",
    "#     preds, model = runXGB(train_X, train_y, test_X, num_rounds=rounds)\n",
    "\n",
    "#     out_df = pd.DataFrame(preds)\n",
    "#     out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "#     out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "#     out_df.to_csv(\"sub_%s.csv\" % filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def output_results(clf, x_test, listing, fname):\n",
    "#     preds = clf.predict_proba(x_test)\n",
    "#     preds = pd.DataFrame(preds)\n",
    "#     cols = ['low', 'medium', 'high']\n",
    "#     preds.columns = cols\n",
    "#     preds['listing_id'] = listing\n",
    "#     preds.to_csv(fname, index=None)\n",
    "#     print(preds[cols].mean().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BaseLine\n",
    "\n",
    "### 1-1. 'price, bathroom, bedroom, features, description, created'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use=[\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\", \"logprice\",\"price_t\",\"price_per_room\", \n",
    "\"num_photos\", \"num_features\", \"num_description_words\",\"listing_id\", \"created_year\", \"created_month\", \"created_day\", \"created_hour\", \n",
    "                 \"created_dayofyear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df[\"bathrooms\"].loc[19671] = 1.5\n",
    "test_df[\"bathrooms\"].loc[22977] = 2.0\n",
    "test_df[\"bathrooms\"].loc[63719] = 2.0\n",
    "train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "\n",
    "train_df[\"logprice\"] = np.log(train_df[\"price\"])\n",
    "test_df[\"logprice\"] = np.log(test_df[\"price\"])\n",
    "\n",
    "train_df[\"price_t\"] =train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "test_df[\"price_t\"] = test_df[\"price\"]/test_df[\"bedrooms\"] \n",
    "\n",
    "train_df['price_t'] = train_df['price_t'].replace(np.inf, train_df['price_t'][train_df['price_t']!=np.inf].mean(), regex=True)\n",
    "test_df['price_t'] = test_df['price_t'].replace(np.inf, test_df['price_t'][test_df['price_t']!=np.inf].mean(), regex=True)\n",
    "\n",
    "train_df['price_t'] = train_df['price_t'].replace(np.NaN, train_df['price_t'][train_df['price_t']!=np.NaN].mean(), regex=True)\n",
    "test_df['price_t'] = test_df['price_t'].replace(np.NaN, test_df['price_t'][test_df['price_t']!=np.NaN].mean(), regex=True)\n",
    "\n",
    "train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "test_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n",
    "\n",
    "train_df['price_per_room'] = train_df['price']/train_df['room_sum']\n",
    "test_df['price_per_room'] = test_df['price']/test_df['room_sum']\n",
    "\n",
    "train_df['price_per_room'] = train_df['price_per_room'].replace(np.inf, train_df['price_per_room'][train_df['price_per_room']!=np.inf].mean(), regex=True)\n",
    "test_df['price_per_room'] = test_df['price_per_room'].replace(np.inf, test_df['price_per_room'][test_df['price_per_room']!=np.inf].mean(), regex=True)\n",
    "\n",
    "train_df['price_per_room'] = train_df['price_per_room'].replace(np.NaN, train_df['price_per_room'][train_df['price_per_room']!=np.NaN].mean(), regex=True)\n",
    "test_df['price_per_room'] = test_df['price_per_room'].replace(np.NaN, test_df['price_per_room'][test_df['price_per_room']!=np.NaN].mean(), regex=True)\n",
    "\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "train_df['created_dayofyear'] = train_df['created'].dt.dayofyear\n",
    "test_df['created_dayofyear'] = test_df['created'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 27) (74659, 26)\n"
     ]
    }
   ],
   "source": [
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.611170811358\n"
     ]
    }
   ],
   "source": [
    "y_val, y_val_pred = runRandomForest(train_df[features_to_use], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.729814608449\n",
      "\n",
      "classification_report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.24      0.33       778\n",
      "          1       0.45      0.30      0.36      2178\n",
      "          2       0.79      0.92      0.85      6915\n",
      "\n",
      "avg / total       0.69      0.73      0.70      9871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.array([np.argmax(i) for i in y_val_pred])\n",
    "pred_y, y = pd.Series(pred_y), pd.Series(y_val)\n",
    "\n",
    "print(\"accuracy :\", accuracy_score(pred_y, y))\n",
    "print(\"\")\n",
    "print(\"classification_report :\\n\", classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08022\ttest-mlogloss:1.08045\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[25]\ttrain-mlogloss:0.806969\ttest-mlogloss:0.813823\n",
      "[50]\ttrain-mlogloss:0.699672\ttest-mlogloss:0.711459\n",
      "[75]\ttrain-mlogloss:0.648552\ttest-mlogloss:0.664082\n",
      "[100]\ttrain-mlogloss:0.619774\ttest-mlogloss:0.639224\n",
      "[125]\ttrain-mlogloss:0.601016\ttest-mlogloss:0.624491\n",
      "[150]\ttrain-mlogloss:0.587251\ttest-mlogloss:0.614851\n",
      "[175]\ttrain-mlogloss:0.57639\ttest-mlogloss:0.608152\n",
      "[200]\ttrain-mlogloss:0.566581\ttest-mlogloss:0.602691\n",
      "[225]\ttrain-mlogloss:0.558082\ttest-mlogloss:0.598578\n",
      "[250]\ttrain-mlogloss:0.550454\ttest-mlogloss:0.595372\n",
      "[275]\ttrain-mlogloss:0.543561\ttest-mlogloss:0.592897\n",
      "[300]\ttrain-mlogloss:0.537081\ttest-mlogloss:0.590663\n",
      "[325]\ttrain-mlogloss:0.53093\ttest-mlogloss:0.588491\n",
      "[350]\ttrain-mlogloss:0.525383\ttest-mlogloss:0.586947\n",
      "[375]\ttrain-mlogloss:0.52007\ttest-mlogloss:0.585838\n",
      "[400]\ttrain-mlogloss:0.515009\ttest-mlogloss:0.584698\n",
      "[425]\ttrain-mlogloss:0.509791\ttest-mlogloss:0.583776\n",
      "[450]\ttrain-mlogloss:0.504849\ttest-mlogloss:0.582806\n",
      "[475]\ttrain-mlogloss:0.500121\ttest-mlogloss:0.58185\n",
      "[500]\ttrain-mlogloss:0.495838\ttest-mlogloss:0.581239\n",
      "[525]\ttrain-mlogloss:0.491406\ttest-mlogloss:0.580604\n",
      "[550]\ttrain-mlogloss:0.487163\ttest-mlogloss:0.580083\n",
      "[575]\ttrain-mlogloss:0.483196\ttest-mlogloss:0.579571\n",
      "[600]\ttrain-mlogloss:0.479018\ttest-mlogloss:0.579077\n",
      "[625]\ttrain-mlogloss:0.475132\ttest-mlogloss:0.57876\n",
      "[650]\ttrain-mlogloss:0.471064\ttest-mlogloss:0.578204\n",
      "[675]\ttrain-mlogloss:0.467216\ttest-mlogloss:0.577839\n",
      "[700]\ttrain-mlogloss:0.463488\ttest-mlogloss:0.577523\n",
      "[725]\ttrain-mlogloss:0.459672\ttest-mlogloss:0.577129\n",
      "[750]\ttrain-mlogloss:0.455987\ttest-mlogloss:0.576918\n",
      "[775]\ttrain-mlogloss:0.452314\ttest-mlogloss:0.576593\n",
      "[800]\ttrain-mlogloss:0.448651\ttest-mlogloss:0.576542\n",
      "[825]\ttrain-mlogloss:0.445249\ttest-mlogloss:0.576368\n",
      "[850]\ttrain-mlogloss:0.441855\ttest-mlogloss:0.576318\n",
      "[875]\ttrain-mlogloss:0.438334\ttest-mlogloss:0.576227\n",
      "[900]\ttrain-mlogloss:0.434722\ttest-mlogloss:0.576309\n",
      "[925]\ttrain-mlogloss:0.431492\ttest-mlogloss:0.576369\n",
      "Stopping. Best iteration:\n",
      "[881]\ttrain-mlogloss:0.43753\ttest-mlogloss:0.576168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X_1 = sparse.coo_matrix(train_df[features_to_use]).tocsr()\n",
    "test_X_1 = sparse.coo_matrix(test_df[features_to_use]).tocsr()\n",
    "\n",
    "val_y, preds = train_xgboost(train_X_1, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.744808023503\n",
      "\n",
      "classification_report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.28      0.38       747\n",
      "          1       0.52      0.34      0.41      2319\n",
      "          2       0.79      0.93      0.86      6805\n",
      "\n",
      "avg / total       0.71      0.74      0.72      9871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.array([np.argmax(i) for i in preds])\n",
    "pred_y, y = pd.Series(pred_y), pd.Series(val_y)\n",
    "\n",
    "print(\"accuracy :\", accuracy_score(pred_y, y))\n",
    "print(\"\")\n",
    "print(\"classification_report :\\n\", classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. latitude, longitude 를 이용한 Feature 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cart2rho(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    return rho\n",
    "\n",
    "\n",
    "def cart2phi(x, y):\n",
    "    phi = np.arctan2(y, x)\n",
    "    return phi\n",
    "\n",
    "\n",
    "def rotation_x(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return x*math.cos(alpha) + y*math.sin(alpha)\n",
    "\n",
    "\n",
    "def rotation_y(row, alpha):\n",
    "    x = row['latitude']\n",
    "    y = row['longitude']\n",
    "    return y*math.cos(alpha) - x*math.sin(alpha)\n",
    "\n",
    "\n",
    "def add_rotation(degrees, df):\n",
    "    namex = \"rot\" + str(degrees) + \"_X\"\n",
    "    namey = \"rot\" + str(degrees) + \"_Y\"\n",
    "\n",
    "    df['num_' + namex] = df.apply(lambda row: rotation_x(row, math.pi/(180/degrees)), axis=1)\n",
    "    df['num_' + namey] = df.apply(lambda row: rotation_y(row, math.pi/(180/degrees)), axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def operate_on_coordinates(tr_df, te_df):\n",
    "    \n",
    "    for df in [tr_df, te_df]:\n",
    "        df[\"num_rho\"] = df.apply(lambda x: cart2rho(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "        df[\"num_phi\"] = df.apply(lambda x: cart2phi(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "    \n",
    "        #rotations\n",
    "        for angle in [15,30,45,60]:\n",
    "            df = add_rotation(angle, df)\n",
    "\n",
    "    return tr_df, te_df\n",
    "\n",
    "train_df, test_df = operate_on_coordinates(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_to_use.extend(['num_rho', 'num_phi', 'num_rot15_X', 'num_rot15_Y', 'num_rot30_X',\n",
    "       'num_rot30_Y', 'num_rot45_X', 'num_rot45_Y', 'num_rot60_X',\n",
    "       'num_rot60_Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_dict = {\n",
    "    'manhattan_loc': [40.728333, -73.994167],\n",
    "    'brooklyn_loc': [40.624722, -73.952222],\n",
    "    'bronx_loc': [40.837222, -73.886111],\n",
    "    'queens_loc': [40.75, -73.866667],\n",
    "    'staten_loc': [40.576281, -74.144839]}\n",
    "\n",
    "for location in location_dict.keys():\n",
    "    dlat = location_dict[location][0] - train_df['latitude']\n",
    "    dlon = (location_dict[location][1] - train_df['longitude']) * np.cos(np.deg2rad(41))  #  adjust for NYC latitude\n",
    "    train_df['distance_' + location] = np.sqrt(dlat ** 2 + dlon ** 2) * 60\n",
    "    \n",
    "    dlat = location_dict[location][0] - test_df['latitude']\n",
    "    dlon = (location_dict[location][1] - test_df['longitude']) * np.cos(np.deg2rad(41))  #  adjust for NYC latitude\n",
    "    test_df['distance_' + location] = np.sqrt(dlat ** 2 + dlon ** 2) * 60     # distance in nautical miles\n",
    "    \n",
    "    features_to_use.append('distance_' + location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"longitude\"] = train_df[\"longitude\"].clip(upper=-60)\n",
    "train_df[\"latitude\"] = train_df[\"latitude\"].clip(lower=35)\n",
    "\n",
    "test_df[\"longitude\"] = test_df[\"longitude\"].clip(upper=-60)\n",
    "test_df[\"latitude\"] = test_df[\"latitude\"].clip(lower=35)\n",
    "\n",
    "train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "\n",
    "train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "features_to_use.append(\"density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 44) (74659, 43)\n"
     ]
    }
   ],
   "source": [
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.616920213711\n"
     ]
    }
   ],
   "source": [
    "val_y, preds = runRandomForest(train_df[features_to_use], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.729814608449\n",
      "\n",
      "classification_report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.27      0.36       807\n",
      "          1       0.47      0.32      0.38      2258\n",
      "          2       0.79      0.92      0.85      6806\n",
      "\n",
      "avg / total       0.70      0.73      0.70      9871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.array([np.argmax(i) for i in preds])\n",
    "pred_y, y = pd.Series(pred_y), pd.Series(val_y)\n",
    "\n",
    "print(\"accuracy :\", accuracy_score(pred_y, y))\n",
    "print(\"\")\n",
    "print(\"classification_report :\\n\", classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.08004\ttest-mlogloss:1.08045\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[25]\ttrain-mlogloss:0.803152\ttest-mlogloss:0.811003\n",
      "[50]\ttrain-mlogloss:0.692695\ttest-mlogloss:0.706735\n",
      "[75]\ttrain-mlogloss:0.639423\ttest-mlogloss:0.658431\n",
      "[100]\ttrain-mlogloss:0.609877\ttest-mlogloss:0.633352\n",
      "[125]\ttrain-mlogloss:0.589778\ttest-mlogloss:0.617866\n",
      "[150]\ttrain-mlogloss:0.57516\ttest-mlogloss:0.608088\n",
      "[175]\ttrain-mlogloss:0.562765\ttest-mlogloss:0.600815\n",
      "[200]\ttrain-mlogloss:0.552071\ttest-mlogloss:0.595401\n",
      "[225]\ttrain-mlogloss:0.542511\ttest-mlogloss:0.590968\n",
      "[250]\ttrain-mlogloss:0.533944\ttest-mlogloss:0.587603\n",
      "[275]\ttrain-mlogloss:0.526819\ttest-mlogloss:0.585477\n",
      "[300]\ttrain-mlogloss:0.51983\ttest-mlogloss:0.583366\n",
      "[325]\ttrain-mlogloss:0.513203\ttest-mlogloss:0.581582\n",
      "[350]\ttrain-mlogloss:0.506984\ttest-mlogloss:0.579844\n",
      "[375]\ttrain-mlogloss:0.501382\ttest-mlogloss:0.578636\n",
      "[400]\ttrain-mlogloss:0.495926\ttest-mlogloss:0.577465\n",
      "[425]\ttrain-mlogloss:0.490734\ttest-mlogloss:0.576653\n",
      "[450]\ttrain-mlogloss:0.485609\ttest-mlogloss:0.575616\n",
      "[475]\ttrain-mlogloss:0.481129\ttest-mlogloss:0.574986\n",
      "[500]\ttrain-mlogloss:0.476326\ttest-mlogloss:0.574144\n",
      "[525]\ttrain-mlogloss:0.471822\ttest-mlogloss:0.573827\n",
      "[550]\ttrain-mlogloss:0.467537\ttest-mlogloss:0.573402\n",
      "[575]\ttrain-mlogloss:0.463117\ttest-mlogloss:0.572877\n",
      "[600]\ttrain-mlogloss:0.459041\ttest-mlogloss:0.572366\n",
      "[625]\ttrain-mlogloss:0.454808\ttest-mlogloss:0.571876\n",
      "[650]\ttrain-mlogloss:0.450777\ttest-mlogloss:0.571614\n",
      "[675]\ttrain-mlogloss:0.446788\ttest-mlogloss:0.571529\n",
      "[700]\ttrain-mlogloss:0.442717\ttest-mlogloss:0.571345\n",
      "[725]\ttrain-mlogloss:0.43881\ttest-mlogloss:0.57102\n",
      "[750]\ttrain-mlogloss:0.434916\ttest-mlogloss:0.570872\n",
      "[775]\ttrain-mlogloss:0.430963\ttest-mlogloss:0.570767\n",
      "[800]\ttrain-mlogloss:0.427506\ttest-mlogloss:0.570589\n",
      "[825]\ttrain-mlogloss:0.423857\ttest-mlogloss:0.570439\n",
      "[850]\ttrain-mlogloss:0.420156\ttest-mlogloss:0.57045\n",
      "[875]\ttrain-mlogloss:0.416533\ttest-mlogloss:0.570431\n",
      "[900]\ttrain-mlogloss:0.413226\ttest-mlogloss:0.570321\n",
      "[925]\ttrain-mlogloss:0.409952\ttest-mlogloss:0.570377\n",
      "[950]\ttrain-mlogloss:0.406756\ttest-mlogloss:0.570087\n",
      "[975]\ttrain-mlogloss:0.403389\ttest-mlogloss:0.570249\n",
      "[1000]\ttrain-mlogloss:0.400099\ttest-mlogloss:0.570096\n",
      "Stopping. Best iteration:\n",
      "[956]\ttrain-mlogloss:0.405931\ttest-mlogloss:0.570068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X_2 = sparse.coo_matrix(train_df[features_to_use]).tocsr()\n",
    "test_X_2 = sparse.coo_matrix(test_df[features_to_use]).tocsr()\n",
    "\n",
    "val_y, preds = train_xgboost(train_X_2, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.746834160673\n",
      "\n",
      "classification_report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.32      0.42       747\n",
      "          1       0.52      0.35      0.42      2319\n",
      "          2       0.80      0.93      0.86      6805\n",
      "\n",
      "avg / total       0.72      0.75      0.72      9871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.array([np.argmax(i) for i in preds])\n",
    "pred_y, y = pd.Series(pred_y), pd.Series(val_y)\n",
    "\n",
    "print(\"accuracy :\", accuracy_score(pred_y, y))\n",
    "print(\"\")\n",
    "print(\"classification_report :\\n\", classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. manager_id, building_id 를 이용한 feature 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. manager_level(low, medium, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    \n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    \n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "            \n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "            \n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. builing_id, manager_id 를 Count 하여 log 값 취한 후 feature 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pre_processing(data):\n",
    "    \n",
    "    build_counts = pd.DataFrame(data.building_id.value_counts())\n",
    "    build_counts['b_counts'] = build_counts['building_id']\n",
    "    build_counts['building_id'] = build_counts.index\n",
    "    build_counts['b_count_log'] = np.log2(build_counts['b_counts'])\n",
    "    data = pd.merge(data, build_counts, on='building_id')\n",
    "    \n",
    "    man_counts = pd.DataFrame(data.manager_id.value_counts())\n",
    "    man_counts['m_counts'] = man_counts['manager_id']\n",
    "    man_counts['manager_id'] = man_counts.index\n",
    "    man_counts['m_count_log'] = np.log10(man_counts['m_counts'])\n",
    "    data = pd.merge(data, man_counts, on='manager_id')\n",
    "    \n",
    "    return data\n",
    "\n",
    "train_df['Source'] = \"train\"\n",
    "test_df['Source'] = \"test\"\n",
    "\n",
    "data = pd.concat([train_df, test_df])\n",
    "\n",
    "pre_data = pre_processing(data)\n",
    "\n",
    "train_df = pre_data[pre_data['Source'] == \"train\"]\n",
    "test_df = pre_data[pre_data['Source'] == \"test\"]\n",
    "\n",
    "features_to_use += ['b_counts', 'm_counts', 'b_count_log', 'm_count_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 52) (74659, 52)\n"
     ]
    }
   ],
   "source": [
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeonju/anaconda3/envs/ml_python/lib/python3.4/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hyeonju/anaconda3/envs/ml_python/lib/python3.4/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hyeonju/anaconda3/envs/ml_python/lib/python3.4/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/hyeonju/anaconda3/envs/ml_python/lib/python3.4/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "feature_append = ['manager_level_low','manager_level_medium','manager_level_high']\n",
    "\n",
    "for name in feature_append:\n",
    "    train_df[name] = train_df[name].replace(np.inf, train_df[name][train_df[name]!=np.inf].mean(), regex=True)\n",
    "    test_df[name] = test_df[name].replace(np.inf, test_df[name][test_df[name]!=np.inf].mean(), regex=True)\n",
    "\n",
    "    train_df[name] = train_df[name].replace(np.NaN, train_df[name][train_df[name]!=np.NaN].mean(), regex=True)\n",
    "    test_df[name] = test_df[name].replace(np.NaN, test_df[name][test_df[name]!=np.NaN].mean(), regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.566731964978\n"
     ]
    }
   ],
   "source": [
    "val_y, preds = runRandomForest(train_df[features_to_use], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.749468138993\n",
      "\n",
      "classification_report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.33      0.42       781\n",
      "          1       0.51      0.37      0.43      2271\n",
      "          2       0.81      0.92      0.86      6819\n",
      "\n",
      "avg / total       0.72      0.75      0.73      9871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.array([np.argmax(i) for i in preds])\n",
    "pred_y, y = pd.Series(pred_y), pd.Series(val_y)\n",
    "\n",
    "print(\"accuracy :\", accuracy_score(pred_y, y))\n",
    "print(\"\")\n",
    "print(\"classification_report :\\n\", classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.07828\ttest-mlogloss:1.07877\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[25]\ttrain-mlogloss:0.773066\ttest-mlogloss:0.782023\n",
      "[50]\ttrain-mlogloss:0.65163\ttest-mlogloss:0.667114\n",
      "[75]\ttrain-mlogloss:0.593965\ttest-mlogloss:0.615374\n",
      "[100]\ttrain-mlogloss:0.562029\ttest-mlogloss:0.588926\n",
      "[125]\ttrain-mlogloss:0.541523\ttest-mlogloss:0.573883\n",
      "[150]\ttrain-mlogloss:0.526102\ttest-mlogloss:0.564002\n",
      "[175]\ttrain-mlogloss:0.513724\ttest-mlogloss:0.556888\n",
      "[200]\ttrain-mlogloss:0.502986\ttest-mlogloss:0.551513\n",
      "[225]\ttrain-mlogloss:0.493432\ttest-mlogloss:0.547248\n",
      "[250]\ttrain-mlogloss:0.48494\ttest-mlogloss:0.544164\n",
      "[275]\ttrain-mlogloss:0.476962\ttest-mlogloss:0.541476\n",
      "[300]\ttrain-mlogloss:0.46935\ttest-mlogloss:0.539069\n",
      "[325]\ttrain-mlogloss:0.46213\ttest-mlogloss:0.536997\n",
      "[350]\ttrain-mlogloss:0.455675\ttest-mlogloss:0.53545\n",
      "[375]\ttrain-mlogloss:0.449702\ttest-mlogloss:0.534141\n",
      "[400]\ttrain-mlogloss:0.443325\ttest-mlogloss:0.532673\n",
      "[425]\ttrain-mlogloss:0.437396\ttest-mlogloss:0.531827\n",
      "[450]\ttrain-mlogloss:0.431948\ttest-mlogloss:0.531049\n",
      "[475]\ttrain-mlogloss:0.426374\ttest-mlogloss:0.530236\n",
      "[500]\ttrain-mlogloss:0.421073\ttest-mlogloss:0.52954\n",
      "[525]\ttrain-mlogloss:0.416141\ttest-mlogloss:0.528907\n",
      "[550]\ttrain-mlogloss:0.411153\ttest-mlogloss:0.528401\n",
      "[575]\ttrain-mlogloss:0.406488\ttest-mlogloss:0.527899\n",
      "[600]\ttrain-mlogloss:0.401856\ttest-mlogloss:0.527394\n",
      "[625]\ttrain-mlogloss:0.397417\ttest-mlogloss:0.527193\n",
      "[650]\ttrain-mlogloss:0.393193\ttest-mlogloss:0.527176\n",
      "[675]\ttrain-mlogloss:0.388573\ttest-mlogloss:0.526706\n",
      "[700]\ttrain-mlogloss:0.384343\ttest-mlogloss:0.526287\n",
      "[725]\ttrain-mlogloss:0.380133\ttest-mlogloss:0.526238\n",
      "[750]\ttrain-mlogloss:0.375869\ttest-mlogloss:0.526058\n",
      "[775]\ttrain-mlogloss:0.37181\ttest-mlogloss:0.526034\n",
      "[800]\ttrain-mlogloss:0.367911\ttest-mlogloss:0.525769\n",
      "[825]\ttrain-mlogloss:0.363676\ttest-mlogloss:0.525583\n",
      "[850]\ttrain-mlogloss:0.359621\ttest-mlogloss:0.525459\n",
      "[875]\ttrain-mlogloss:0.355687\ttest-mlogloss:0.525459\n",
      "[900]\ttrain-mlogloss:0.351656\ttest-mlogloss:0.525293\n",
      "[925]\ttrain-mlogloss:0.347738\ttest-mlogloss:0.525292\n",
      "[950]\ttrain-mlogloss:0.343973\ttest-mlogloss:0.525307\n",
      "[975]\ttrain-mlogloss:0.340406\ttest-mlogloss:0.525163\n",
      "[1000]\ttrain-mlogloss:0.336997\ttest-mlogloss:0.525212\n",
      "[1025]\ttrain-mlogloss:0.333618\ttest-mlogloss:0.525265\n",
      "Stopping. Best iteration:\n",
      "[984]\ttrain-mlogloss:0.339027\ttest-mlogloss:0.525082\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_X_3 = sparse.coo_matrix(train_df[features_to_use]).tocsr()\n",
    "test_X_3 = sparse.coo_matrix(test_df[features_to_use]).tocsr()\n",
    "\n",
    "val_y, preds = train_xgboost(train_X_3, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.763347178604\n",
      "\n",
      "classification_report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.60      0.35      0.44       775\n",
      "          1       0.53      0.44      0.48      2256\n",
      "          2       0.83      0.92      0.87      6840\n",
      "\n",
      "avg / total       0.74      0.76      0.75      9871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.array([np.argmax(i) for i in preds])\n",
    "pred_y, y = pd.Series(pred_y), pd.Series(val_y)\n",
    "\n",
    "print(\"accuracy :\", accuracy_score(pred_y, y))\n",
    "print(\"\")\n",
    "print(\"classification_report :\\n\", classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Date 와 display_address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. 방에 대한 사진이 생성된 날짜를 Feature 로 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_date.columns = [\"listing_id\", \"time_stamp\"]\n",
    "image_date.loc[80240,\"time_stamp\"] = 1478129766 \n",
    "\n",
    "image_date[\"img_date\"] = pd.to_datetime(image_date[\"time_stamp\"], unit=\"s\")\n",
    "image_date[\"img_days_passed\"] = (image_date[\"img_date\"].max() - image_date[\"img_date\"]).astype(\"timedelta64[D]\").astype(int)\n",
    "image_date[\"img_date_month\"] = image_date[\"img_date\"].dt.month\n",
    "image_date[\"img_date_week\"] = image_date[\"img_date\"].dt.week\n",
    "image_date[\"img_date_day\"] = image_date[\"img_date\"].dt.day\n",
    "image_date[\"img_date_dayofweek\"] = image_date[\"img_date\"].dt.dayofweek\n",
    "image_date[\"img_date_dayofyear\"] = image_date[\"img_date\"].dt.dayofyear\n",
    "image_date[\"img_date_hour\"] = image_date[\"img_date\"].dt.hour\n",
    "image_date[\"img_date_monthBeginMidEnd\"] = image_date[\"img_date_day\"].apply(lambda x: 1 if x<10 else 2 if x<20 else 3)\n",
    "\n",
    "train_df = pd.merge(train_df, image_date, on=\"listing_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, image_date, on=\"listing_id\", how=\"left\")\n",
    "\n",
    "img_date_column = image_date.columns.values.tolist()\n",
    "\n",
    "img_date_column.remove('listing_id')\n",
    "img_date_column.remove('time_stamp')\n",
    "img_date_column.remove('img_date')\n",
    "\n",
    "features_to_use += img_date_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. display address 를 같은 형태로 변환\n",
    "\n",
    "- W 13 Street, West 13 Street\n",
    "- Columbus Avenue, columbus ave 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "train_df['display_address'] = train_df['display_address'].apply(lambda x: x.lower())\n",
    "train_df['display_address'] = train_df['display_address'].apply(lambda x: re.sub(\"[^a-zA-Z0-9 ]\", \"\", x))\n",
    "train_df['display_address'] = train_df['display_address'].apply(lambda x: re.sub(\"street\", \"st\", x))\n",
    "train_df['display_address'] = train_df['display_address'].apply(lambda x: re.sub(\"west\", \"w\", x))\n",
    "train_df['display_address'] = train_df['display_address'].apply(lambda x: re.sub(\"east\", \"e\", x))\n",
    "train_df['display_address'] = train_df['display_address'].apply(lambda x: re.sub(\"avenue\", \"ave\", x))\n",
    "train_df['display_address'] = train_df['display_address'].apply(lambda x: (\" \".join(x.split())).strip())\n",
    "\n",
    "test_df['display_address'] = test_df['display_address'].apply(lambda x: x.lower())\n",
    "test_df['display_address'] = test_df['display_address'].apply(lambda x: re.sub(\"[^a-zA-Z0-9 ]\", \"\", x))\n",
    "test_df['display_address'] = test_df['display_address'].apply(lambda x: re.sub(\"street\", \"st\", x))\n",
    "test_df['display_address'] = test_df['display_address'].apply(lambda x: re.sub(\"west\", \"w\", x))\n",
    "test_df['display_address'] = test_df['display_address'].apply(lambda x: re.sub(\"east\", \"e\", x))\n",
    "test_df['display_address'] = test_df['display_address'].apply(lambda x: re.sub(\"avenue\", \"ave\", x))\n",
    "test_df['display_address'] = test_df['display_address'].apply(lambda x: (\" \".join(x.split())).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_Y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.557704508563\n"
     ]
    }
   ],
   "source": [
    "y_val, y_val_pred = runRandomForest(train_df[features_to_use], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.750177287002\n",
      "\n",
      "classification_report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.56      0.27      0.36       794\n",
      "          1       0.50      0.38      0.43      2206\n",
      "          2       0.81      0.93      0.86      6871\n",
      "\n",
      "avg / total       0.72      0.75      0.73      9871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.array([np.argmax(i) for i in y_val_pred])\n",
    "pred_y, y = pd.Series(pred_y), pd.Series(y_val)\n",
    "\n",
    "print(\"accuracy :\", accuracy_score(pred_y, y))\n",
    "print(\"\")\n",
    "print(\"classification_report :\\n\", classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.07758\ttest-mlogloss:1.078\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[25]\ttrain-mlogloss:0.766519\ttest-mlogloss:0.775943\n",
      "[50]\ttrain-mlogloss:0.641937\ttest-mlogloss:0.659129\n",
      "[75]\ttrain-mlogloss:0.580935\ttest-mlogloss:0.604771\n",
      "[100]\ttrain-mlogloss:0.546446\ttest-mlogloss:0.576634\n",
      "[125]\ttrain-mlogloss:0.523436\ttest-mlogloss:0.559693\n",
      "[150]\ttrain-mlogloss:0.506475\ttest-mlogloss:0.548775\n",
      "[175]\ttrain-mlogloss:0.492295\ttest-mlogloss:0.540765\n",
      "[200]\ttrain-mlogloss:0.480482\ttest-mlogloss:0.534561\n",
      "[225]\ttrain-mlogloss:0.469915\ttest-mlogloss:0.529671\n",
      "[250]\ttrain-mlogloss:0.460963\ttest-mlogloss:0.52568\n",
      "[275]\ttrain-mlogloss:0.452436\ttest-mlogloss:0.522165\n",
      "[300]\ttrain-mlogloss:0.444752\ttest-mlogloss:0.519491\n",
      "[325]\ttrain-mlogloss:0.43742\ttest-mlogloss:0.517305\n",
      "[350]\ttrain-mlogloss:0.43016\ttest-mlogloss:0.51531\n",
      "[375]\ttrain-mlogloss:0.423417\ttest-mlogloss:0.513431\n",
      "[400]\ttrain-mlogloss:0.417223\ttest-mlogloss:0.512017\n",
      "[425]\ttrain-mlogloss:0.41122\ttest-mlogloss:0.510567\n",
      "[450]\ttrain-mlogloss:0.405582\ttest-mlogloss:0.50946\n",
      "[475]\ttrain-mlogloss:0.399896\ttest-mlogloss:0.508486\n",
      "[500]\ttrain-mlogloss:0.394245\ttest-mlogloss:0.507616\n",
      "[525]\ttrain-mlogloss:0.388767\ttest-mlogloss:0.506833\n",
      "[550]\ttrain-mlogloss:0.383744\ttest-mlogloss:0.506192\n",
      "[575]\ttrain-mlogloss:0.378708\ttest-mlogloss:0.505472\n",
      "[600]\ttrain-mlogloss:0.37387\ttest-mlogloss:0.504857\n",
      "[625]\ttrain-mlogloss:0.369296\ttest-mlogloss:0.504365\n",
      "[650]\ttrain-mlogloss:0.364939\ttest-mlogloss:0.503944\n",
      "[675]\ttrain-mlogloss:0.360074\ttest-mlogloss:0.50343\n",
      "[700]\ttrain-mlogloss:0.355953\ttest-mlogloss:0.503137\n",
      "[725]\ttrain-mlogloss:0.35141\ttest-mlogloss:0.502937\n",
      "[750]\ttrain-mlogloss:0.347309\ttest-mlogloss:0.50267\n",
      "[775]\ttrain-mlogloss:0.343229\ttest-mlogloss:0.502496\n",
      "[800]\ttrain-mlogloss:0.339083\ttest-mlogloss:0.502251\n",
      "[825]\ttrain-mlogloss:0.335259\ttest-mlogloss:0.502039\n",
      "[850]\ttrain-mlogloss:0.331031\ttest-mlogloss:0.501892\n",
      "[875]\ttrain-mlogloss:0.326939\ttest-mlogloss:0.501599\n",
      "[900]\ttrain-mlogloss:0.32313\ttest-mlogloss:0.50154\n",
      "[925]\ttrain-mlogloss:0.319068\ttest-mlogloss:0.501674\n",
      "[950]\ttrain-mlogloss:0.315026\ttest-mlogloss:0.501555\n",
      "[975]\ttrain-mlogloss:0.311238\ttest-mlogloss:0.501573\n",
      "[1000]\ttrain-mlogloss:0.307498\ttest-mlogloss:0.501519\n",
      "Stopping. Best iteration:\n",
      "[954]\ttrain-mlogloss:0.314459\ttest-mlogloss:0.50149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_y, preds = train_xgboost(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.779252355384\n",
      "\n",
      "classification_report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      0.38      0.47       775\n",
      "          1       0.56      0.48      0.51      2256\n",
      "          2       0.84      0.93      0.88      6840\n",
      "\n",
      "avg / total       0.76      0.78      0.77      9871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y = np.array([np.argmax(i) for i in preds])\n",
    "pred_y, y = pd.Series(pred_y), pd.Series(val_y)\n",
    "\n",
    "print(\"accuracy :\", accuracy_score(pred_y, y))\n",
    "print(\"\")\n",
    "print(\"classification_report :\\n\", classification_report(y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과(Log Loss)\n",
    "\n",
    " * * * \n",
    "|                       |              Feature 1 |            Feature 2 | Feature 3              | Feature 4             |\n",
    "| :---:                  | :---:                       | :---:                       | :---:                      | :---:                     |\n",
    "|Random Forest |0.614368474766 | 0.616603386848|0.560194517882 |0.54787088605 |\n",
    "|XGBoost           | 0.576168            |0.57073              |0.527771              | 0.50247           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kaggle_submission](./hyeonju_submission.png)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
