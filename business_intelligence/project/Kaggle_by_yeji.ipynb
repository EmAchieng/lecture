{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle\n",
    "### Two Sigma Connect : Rental Listing Inquiries\n",
    "\n",
    "Finding the perfect place to call your new home should be more than browsing through endless listings.<br> \n",
    "RentHop makes apartment search smarter by using data to sort rental listings by quality. <br>\n",
    "Two Sigma Ventures, invite Kagglers to unleash their creative engines to uncover business value in this unique recruiting competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_json(\"train.json\")\n",
    "test_set = pd.read_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=321, num_rounds=2000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.02\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Features (1)\n",
    "Logprice, Bedrooms per price, Total number of rooms, Rooms per price\n",
    "In bathrooms column, some test dataset has different value compared with description column. You can see using .iloc\n",
    "<br>\n",
    "Let's change some values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set[\"bathrooms\"].loc[19671] = 1.5\n",
    "test_set[\"bathrooms\"].loc[22977] = 2.0\n",
    "test_set[\"bathrooms\"].loc[63719] = 2.0\n",
    "\n",
    "train_set[\"logprice\"] = np.log(train_set[\"price\"])\n",
    "test_set[\"logprice\"] = np.log(test_set[\"price\"])\n",
    "\n",
    "train_set[\"per_bed\"] =train_set[\"price\"]/train_set[\"bedrooms\"]\n",
    "test_set[\"per_bed\"] = test_set[\"price\"]/test_set[\"bedrooms\"] \n",
    "\n",
    "train_set[\"room_sum\"] = train_set[\"bedrooms\"]+train_set[\"bathrooms\"] \n",
    "test_set[\"room_sum\"] = test_set[\"bedrooms\"]+test_set[\"bathrooms\"] \n",
    "\n",
    "train_set['per_room'] = train_set['price']/train_set['room_sum']\n",
    "test_set['per_room'] = test_set['price']/test_set['room_sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Features (2)\n",
    "Measure len of photos,features,decription values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set[\"num_photos\"] = train_set[\"photos\"].apply(len)\n",
    "test_set[\"num_photos\"] = test_set[\"photos\"].apply(len)\n",
    "\n",
    "train_set[\"num_features\"] = train_set[\"features\"].apply(len)\n",
    "test_set[\"num_features\"] = test_set[\"features\"].apply(len)\n",
    "\n",
    "train_set[\"num_description_words\"] = train_set[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_set[\"num_description_words\"] = test_set[\"description\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Features (3)\n",
    "Create_year,Create_month,Create_day and Create_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set[\"created\"] = pd.to_datetime(train_set[\"created\"])\n",
    "test_set[\"created\"] = pd.to_datetime(test_set[\"created\"])\n",
    "\n",
    "train_set[\"created_year\"] = train_set[\"created\"].dt.year\n",
    "test_set[\"created_year\"] = test_set[\"created\"].dt.year\n",
    "\n",
    "train_set[\"created_month\"] = train_set[\"created\"].dt.month\n",
    "test_set[\"created_month\"] = test_set[\"created\"].dt.month\n",
    "\n",
    "train_set[\"created_day\"] = train_set[\"created\"].dt.day\n",
    "test_set[\"created_day\"] = test_set[\"created\"].dt.day\n",
    "\n",
    "train_set[\"created_hour\"] = train_set[\"created\"].dt.hour\n",
    "test_set[\"created_hour\"] = test_set[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Features (4)\n",
    "Density, Longitude and Latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set[\"pos\"] = train_set.longitude.round(3).astype(str) + '_' + train_set.latitude.round(3).astype(str)\n",
    "test_set[\"pos\"] = test_set.longitude.round(3).astype(str) + '_' + test_set.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_set['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_set[\"density\"] = train_set['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_set[\"density\"] = test_set['pos'].apply(lambda x: dvals.get(x, vals.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_use=[\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\"per_bed\",\"per_room\", \"logprice\", \"density\",\n",
    "\"num_photos\", \"num_features\", \"num_description_words\",\"listing_id\", \"created_year\", \"created_month\", \"created_day\", \"created_hour\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Features (5)\n",
    "Change word in display_address column and add manager_level column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=list(range(train_set.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_set)\n",
    "b=[np.nan]*len(train_set)\n",
    "c=[np.nan]*len(train_set)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_set['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    \n",
    "    test_index=index[int((i*train_set.shape[0])/5):int(((i+1)*train_set.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    \n",
    "    for j in train_index:\n",
    "        temp=train_set.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "            \n",
    "    for j in test_index:\n",
    "        temp=train_set.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "            \n",
    "train_set['manager_level_low']=a\n",
    "train_set['manager_level_medium']=b\n",
    "train_set['manager_level_high']=c\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_set['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "\n",
    "for j in range(train_set.shape[0]):\n",
    "    temp=train_set.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_set['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_set['manager_level_low']=a\n",
    "test_set['manager_level_medium']=b\n",
    "test_set['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set['address'] = train_set['display_address'].apply(lambda x: x.lower())\n",
    "test_set['address'] = test_set['display_address'].apply(lambda x: x.lower())\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "address_map = {\n",
    "    'w': 'west',\n",
    "    'st.': 'street',\n",
    "    'ave': 'avenue',\n",
    "    'st': 'street',\n",
    "    'e': 'east',\n",
    "    'n': 'north',\n",
    "    's': 'south'\n",
    "}\n",
    "\n",
    "\n",
    "def address_map_func(s):\n",
    "    s = s.split(' ')\n",
    "    out = []\n",
    "    for x in s:\n",
    "        if x in address_map:\n",
    "            out.append(address_map[x])\n",
    "        else:\n",
    "            out.append(x)\n",
    "    return ' '.join(out)\n",
    "\n",
    "train_set['address'] = train_set['address'].apply(lambda x: x.translate(remove_punct_map))\n",
    "test_set['address'] = test_set['address'].apply(lambda x: address_map_func(x))\n",
    "\n",
    "train_set['address'] = train_set['address'].apply(lambda x: x.translate(remove_punct_map))\n",
    "test_set['address'] = test_set['address'].apply(lambda x: address_map_func(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical = [\"address\", \"manager_id\", \"building_id\"]\n",
    "for f in categorical:\n",
    "        if train_set[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_set[f].values) + list(test_set[f].values))\n",
    "            train_set[f] = lbl.transform(list(train_set[f].values))\n",
    "            test_set[f] = lbl.transform(list(test_set[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set['features'] = train_set[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_set['features'] = test_set[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_set[\"features\"])\n",
    "te_sparse = tfidf.transform(test_set[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = sparse.hstack([train_set[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_set[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_set['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=2000)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_set.listing_id.values\n",
    "out_df.to_csv(\"final_yed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![점수](https://github.com/yejiiii/Competition_2017/blob/master/kaggle_score.png?raw=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
